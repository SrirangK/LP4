{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "710d8d9a-53f7-4143-b264-37a759acdbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Lambda, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a5ec8e4-8802-4d1e-96dc-80bb18ec42f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\srira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\srira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# A\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "with open(\"CBOW.txt\", \"r\") as file:\n",
    "    text = file.read().lower()\n",
    "\n",
    "# Tokenize text\n",
    "nltk.download(\"punkt\")\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Remove punctuation and short tokens\n",
    "tokens = [t for t in tokens if t.isalpha() and t not in stopwords]\n",
    "\n",
    "# Create word index mapping\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokens)\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ffbed27-8f7a-4b33-8f60-684e4567ae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 83\n"
     ]
    }
   ],
   "source": [
    "# b. GENERATE TRAINING DATA\n",
    "# -------------------------------\n",
    "\n",
    "def generate_cbow_data(words, window_size):\n",
    "    data = []\n",
    "    for i in range(window_size, len(words) - window_size):\n",
    "        context = []\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if j != 0:\n",
    "                context.append(word2idx[words[i + j]])\n",
    "        target = word2idx[words[i]]\n",
    "        data.append((context, target))\n",
    "    return data\n",
    "\n",
    "window_size = 2\n",
    "data = generate_cbow_data(tokens, window_size)\n",
    "\n",
    "# Extract context and target words\n",
    "contexts = [x[0] for x in data]\n",
    "targets = [x[1] for x in data]\n",
    "\n",
    "# Pad contexts to have uniform length (4 words when window_size=2)\n",
    "X = pad_sequences(contexts, maxlen=2 * window_size, padding='pre')\n",
    "y = to_categorical(targets, num_classes=vocab_size)\n",
    "\n",
    "print(\"Training samples:\", X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8d41a-f6b4-4a42-80d7-dc75b59edf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. TRAIN MODEL\n",
    "# -------------------------------\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=2 * window_size))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_dim,)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=1, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f9182c2-b178-4303-a29f-4fd1059ac655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Embeddings (first 10 words):\n",
      "\n",
      "transmission: [-0.13669029 -0.2356931  -0.23462848  0.22132131 -0.09976947 -0.24282283\n",
      " -0.0304919   0.05004571 -0.14603662 -0.10281289]\n",
      "influenza: [-0.16292754  0.01356544 -0.00779893 -0.11601117 -0.03903944  0.19749321\n",
      " -0.00531609 -0.11682897  0.11274537 -0.14728487]\n",
      "virus: [ 0.10860355 -0.11521085  0.17946602  0.0419849  -0.22062562  0.16980483\n",
      "  0.21313797 -0.10354988 -0.25543106  0.23004846]\n",
      "serial: [ 0.20319796 -0.02197481  0.18700454  0.25183597 -0.19920234  0.25551102\n",
      "  0.12720193 -0.25603944 -0.17428419  0.01812837]\n",
      "interval: [ 0.18431884  0.2111258   0.13806671  0.0203828  -0.23104802  0.19944033\n",
      "  0.176656   -0.16870098 -0.06027011 -0.05120527]\n",
      "days: [ 0.09380592 -0.22281852  0.16495118 -0.05796074 -0.21057105  0.18846947\n",
      "  0.25903335 -0.16117305 -0.14271003 -0.18910405]\n",
      "viruses: [ 0.12073462  0.1060987   0.2137282  -0.18995665 -0.07419154  0.19145706\n",
      "  0.13143004 -0.09222515  0.11901081 -0.18350185]\n",
      "shorter: [ 0.10283981  0.04082376  0.00785166 -0.08494812 -0.05872432  0.23953317\n",
      "  0.04930156 -0.18881759 -0.17403181  0.16284092]\n",
      "time: [ 0.03218586  0.20574948  0.04994145  0.0032903  -0.14940183  0.0582167\n",
      "  0.10106608 -0.19136916 -0.17028767  0.1865074 ]\n",
      "appearance: [ 0.25721815  0.09898578  0.05587003  0.16031566 -0.18016133  0.03768684\n",
      " -0.18560198  0.11853326 -0.18648127  0.17453372]\n"
     ]
    }
   ],
   "source": [
    "# d. OUTPUT\n",
    "# -------------------------------\n",
    "# Display learned embeddings for a few words\n",
    "\n",
    "embeddings = model.layers[0].get_weights()[0]\n",
    "\n",
    "# Choose a few words to display\n",
    "sample_words = list(word2idx.keys())[:10]\n",
    "print(\"\\nWord Embeddings (first 10 words):\\n\")\n",
    "for word in sample_words:\n",
    "    print(f\"{word}: {embeddings[word2idx[word]][:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa13c310-ffc5-4721-8370-7ee284a18c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the most likely target word\n",
    "def predict_target_word(context_words, tokenizer, model, window_size=2):\n",
    "    # Convert words to indices\n",
    "    context_indices = [tokenizer.word_index.get(w, 0) for w in context_words if w in tokenizer.word_index]\n",
    "    \n",
    "    # Pad to required context size\n",
    "    X = pad_sequences([context_indices], maxlen=2 * window_size, padding='pre')\n",
    "    \n",
    "    # Predict probabilities for all words\n",
    "    prediction = model.predict(X, verbose=0)\n",
    "    \n",
    "    # Get index of most probable word\n",
    "    target_index = np.argmax(prediction)\n",
    "    \n",
    "    # Convert index back to word\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == target_index:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9957bce9-4ac4-4b5b-b107-1193f7aca1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 61.45%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def evaluate_accuracy(model, X, y_true):\n",
    "    # Predict probabilities for all samples\n",
    "    y_pred = model.predict(X, verbose=0)\n",
    "    \n",
    "    # Convert predicted probabilities to class indices\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Convert true one-hot vectors to class indices\n",
    "    y_true_classes = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    # Compare predictions with true labels\n",
    "    accuracy = np.mean(y_pred_classes == y_true_classes)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate on training data\n",
    "accuracy = evaluate_accuracy(model, X, y)\n",
    "print(f\"Training Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720855d5-6833-47ef-90c0-b0797cb2d46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
